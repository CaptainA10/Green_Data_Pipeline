# IntÃ©gration Apache Airflow - Green Energy Pipeline

Ce dossier contient la configuration complÃ¨te pour orchestrer le pipeline de donnÃ©es Green Energy avec Apache Airflow, sÃ©curisÃ© par une clÃ© de service GCP.

## ðŸ“‚ Structure des Fichiers

```
airflow/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ green_energy_dag.py       # Le DAG principal (Ingest -> Prep -> Load -> dbt)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ fetch_weather.py          # RÃ©cupÃ©ration API Open-Meteo (JSON brut)
â”‚   â”œâ”€â”€ process_weather.py        # Nettoyage et conversion CSV
â”‚   â””â”€â”€ load_weather_to_bq.py     # Chargement BigQuery
â”œâ”€â”€ keys/
â”‚   â””â”€â”€ service-account.json      # VOTRE CLÃ‰ GCP (Ã  ajouter, ignorÃ©e par git)
â”œâ”€â”€ docker-compose.yaml           # Orchestration (Webserver, Scheduler, Postgres)
â”œâ”€â”€ Dockerfile                    # Image personnalisÃ©e (Airflow + dbt + GCP libs)
â”œâ”€â”€ profiles.yml                  # Configuration dbt pour Airflow
â”œâ”€â”€ requirements.txt              # DÃ©pendances Python
â””â”€â”€ README.md                     # Ce fichier
```

## ðŸ”‘ Configuration de la ClÃ© GCP

**CRITIQUE :** Vous devez placer votre fichier de clÃ© JSON dans le dossier `keys/`.

1.  CrÃ©ez le dossier s'il n'existe pas :
    ```bash
    mkdir -p airflow/keys
    ```
2.  Copiez votre clÃ© JSON Ã  l'intÃ©rieur et renommez-la **exactement** en `service-account.json`.
    - Chemin final : `airflow/keys/service-account.json`
3.  **SÃ©curitÃ©** : Ce fichier est automatiquement ignorÃ© par `.gitignore` (si configurÃ© Ã  la racine) pour ne pas Ãªtre commitÃ©. VÃ©rifiez votre `.gitignore` racine :
    ```text
    airflow/keys/
    *.json
    ```

## ðŸš€ Lancement du Pipeline

1.  **Construction de l'image** :
    ```bash
    cd airflow
    docker-compose build
    ```

2.  **Initialisation** (Premier lancement uniquement) :
    ```bash
    docker-compose up airflow-init
    ```

3.  **DÃ©marrage** :
    ```bash
    docker-compose up -d
    ```

4.  **AccÃ¨s UI** : [http://localhost:8080](http://localhost:8080) (Logins: `admin` / `admin`).

## ðŸ§ª Test du Pipeline

1.  Activez le DAG `green_energy_pipeline` dans l'interface.
2.  DÃ©clenchez-le manuellement (Bouton "Trigger DAG").
3.  VÃ©rifiez les logs :
    - **Ingest** : Doit sauvegarder `raw_weather.json`.
    - **Prepare** : Doit crÃ©er `weather_data.csv`.
    - **Load** : Doit charger dans `effidic-stage-2026.raw_energy.daily_weather`.
    - **dbt** : Doit exÃ©cuter `dbt run` et `dbt test` avec succÃ¨s.

## ðŸ›  DÃ©tails Techniques

- **Source** : Open-Meteo API (DonnÃ©es mÃ©tÃ©o Paris, historiques et journaliÃ¨res).
- **BigQuery** : Table `raw_energy.daily_weather` (Mode `WRITE_TRUNCATE` pour la dÃ©mo, n'Ã©crase pas vos autres tables).
- **dbt** : Utilise le profil `green_energy` dÃ©fini dans `airflow/profiles.yml`, pointant vers le dataset `green_energy_dbt` (ou celui configurÃ©).
